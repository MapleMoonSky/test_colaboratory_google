{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER2013",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleMoonSky/test_colaboratory_google/blob/master/FER2013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFm8PFVIdFP_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0df44a5e-c3d6-4369-c7ce-835088fd6028"
      },
      "source": [
        "#@title Fer2013 \n",
        "'''\n",
        "训练集：fer28709\n",
        "验证集：3588\n",
        "测试集：3588\n",
        "\n",
        "1-1:\n",
        "Test loss: 3.510471635837619\n",
        "Test accuracy: 0.600613154960981\n",
        "1-2:\n",
        "Test loss: 3.490228995298728\n",
        "Test accuracy: 0.5983835005574136\n",
        "1-3:\n",
        "Test loss: 3.5108614084845535\n",
        "Test accuracy: 0.6064659977703456\n",
        "\n",
        "tv：\n",
        "1-3\n",
        "Test loss: 3.484443254845701\n",
        "Test accuracy: 0.6130471145887377\n",
        "1-4:\n",
        "Test loss: 3.4791799931223175\n",
        "Test accuracy: 0.6201226309921962\n",
        "1-5:\n",
        "Test loss: 3.428256825858534\n",
        "Test accuracy: 0.6089743589743589\n",
        "\n",
        "2019/08/21/001\n",
        "Test loss: 3.3394725171219943\n",
        "Test accuracy: 0.6011705685618729\n",
        "\n",
        "2019/08/21/002\n",
        "Test loss: 3.4780489210301018\n",
        "Test accuracy: 0.6117614269788183\n",
        "\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import string,os,sys\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import random\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "#最多占gpu资源的70%\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
        "#开始不会给tensorflow全部gpu资源 而是按需增加\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D,AveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import TensorBoard\n",
        "#K.set_image_dim_ordering('tf')\n",
        "# the data, shuffled and split between train and test setstest\n",
        "\n",
        "\n",
        "file_path='/content/drive/My Drive/test/fer2013.csv'\n",
        "data=pd.read_csv(file_path,dtype='a')\n",
        "label=np.array(data['emotion'])\n",
        "img_data=np.array(data['pixels'])\n",
        "N_sample = label.size\n",
        "Face_data = np.zeros((N_sample, 48*48))\n",
        "Face_label = np.zeros((N_sample,7), dtype=int)#7列\n",
        "\n",
        "for i in range(N_sample):\n",
        " x = img_data[i]\n",
        " x = np.fromstring(x, dtype=float, sep=' ')\n",
        " x_max = x.max()\n",
        " x = x/(x_max+0.0001)#归一化\n",
        " Face_data[i] = x\n",
        " #重新编码，将类别向量（从0到nb_classes的整数向量）映射为二值类别矩阵\n",
        " z=int(label[i])\n",
        " Face_label[i,z]=1###二值化\n",
        " #img_x =np.reshape(x,(48, 48))\n",
        " #plt.subplot(5,5,i+1);\n",
        " #plt.imshow(img_x,cmap='gray')\n",
        "#plt.show()\n",
        "\n",
        "train_num=28709\n",
        "valid_num=32298\n",
        "test_num=35887\n",
        "#train_num = 28710\n",
        "#test_num = 3590\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_x = Face_data [0:valid_num, :]#分割训练集数据\n",
        "train_y = Face_label [0:valid_num, :]#分割训练集label\n",
        "valid_x = Face_data [train_num+1:valid_num, :]\n",
        "valid_y = Face_label [train_num+1:valid_num, :]\n",
        "\n",
        "test_x = Face_data[valid_num+1:test_num+1, :]#分割测试集数据\n",
        "test_y = Face_label[valid_num+1:test_num+1, :]#分割测试集label\n",
        "\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 48, 48\n",
        "\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = train_x.reshape(train_x.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = test_x.reshape(test_x.shape[0], 1, img_rows, img_cols)\n",
        "    x_valid=valid_x.reshape(valid_x.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = train_x.reshape(train_x.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = test_x.reshape(test_x.shape[0], img_rows, img_cols, 1)\n",
        "    x_valid = valid_x.reshape(valid_x.shape[0],img_rows, img_cols,1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_valid= x_valid.astype('float32')\n",
        "\n",
        "\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_valid.shape[0],'valid samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "y_train=train_y\n",
        "y_test=test_y\n",
        "y_valid=valid_y\n",
        "\n",
        "# 将类别向量（从0到nb_classes的整数向量）映射为二值类别矩阵\n",
        "#用编码器ong-hot重新编码如  9 编码为000000001\n",
        "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 7\n",
        "epochs = 200\n",
        "model = Sequential()\n",
        "\n",
        "#\"convolution2d_10\n",
        "model.add(Conv2D(64, kernel_size=(5, 5),\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape,\n",
        "                 data_format='channels_last'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),data_format='channels_last',padding='valid'))\n",
        "\n",
        "\n",
        "#\"convolution2d_11\n",
        "model.add(Conv2D(64, kernel_size=(5, 5), padding='same',activation='relu',data_format='channels_last'))\n",
        "\n",
        "model.add(AveragePooling2D(pool_size=(3, 3),strides=(2,2),data_format='channels_last',padding='valid'))\n",
        "\n",
        "#\"convolution2d_12\n",
        "model.add(Conv2D(128, kernel_size=(5, 5), padding='same',activation='relu',data_format='channels_last'))\n",
        "\n",
        "#maxpooling2d_4\n",
        "model.add(AveragePooling2D(pool_size=(3, 3),strides=(2,2),data_format='channels_last',padding='valid'))\n",
        "\n",
        "#convolution2d_13\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#dense_4(设置输出层的维度)\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "#dropout_3\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#dense_5稠密层\n",
        "\n",
        "\n",
        "#dense_6\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(lr=0.0010000000474974513,beta_1=0.8999999761581421,beta_2=0.9990000128746033,epsilon=1e-08),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "json_string=model.to_json()\n",
        "open('/content/drive/My Drive/test/model1/model1-5.json','w').write(json_string)\n",
        "model.save_weights('/content/drive/My Drive/test/model1/model1-5.h5')\n",
        "plot_model(model,to_file='/content/drive/My Drive/test/model1/model.png',show_shapes=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (32298, 48, 48, 1)\n",
            "32298 train samples\n",
            "3588 valid samples\n",
            "3588 test samples\n",
            "Train on 32298 samples, validate on 3588 samples\n",
            "Epoch 1/200\n",
            "32298/32298 [==============================] - 10s 323us/step - loss: 1.8044 - acc: 0.2487 - val_loss: 1.6983 - val_acc: 0.3258\n",
            "Epoch 2/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 1.6521 - acc: 0.3492 - val_loss: 1.5292 - val_acc: 0.4055\n",
            "Epoch 3/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 1.5356 - acc: 0.4079 - val_loss: 1.4347 - val_acc: 0.4457\n",
            "Epoch 4/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 1.4389 - acc: 0.4485 - val_loss: 1.3833 - val_acc: 0.4574\n",
            "Epoch 5/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 1.3710 - acc: 0.4773 - val_loss: 1.2990 - val_acc: 0.4972\n",
            "Epoch 6/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 1.3206 - acc: 0.4976 - val_loss: 1.2426 - val_acc: 0.5229\n",
            "Epoch 7/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 1.2747 - acc: 0.5198 - val_loss: 1.2134 - val_acc: 0.5281\n",
            "Epoch 8/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 1.2345 - acc: 0.5348 - val_loss: 1.1727 - val_acc: 0.5454\n",
            "Epoch 9/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 1.2015 - acc: 0.5461 - val_loss: 1.1495 - val_acc: 0.5582\n",
            "Epoch 10/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 1.1697 - acc: 0.5605 - val_loss: 1.1524 - val_acc: 0.5571\n",
            "Epoch 11/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 1.1449 - acc: 0.5671 - val_loss: 1.1322 - val_acc: 0.5672\n",
            "Epoch 12/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 1.1160 - acc: 0.5781 - val_loss: 1.1237 - val_acc: 0.5694\n",
            "Epoch 13/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 1.0918 - acc: 0.5871 - val_loss: 1.1102 - val_acc: 0.5766\n",
            "Epoch 14/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 1.0718 - acc: 0.5974 - val_loss: 1.1017 - val_acc: 0.5769\n",
            "Epoch 15/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 1.0439 - acc: 0.6060 - val_loss: 1.0879 - val_acc: 0.5819\n",
            "Epoch 16/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 1.0202 - acc: 0.6120 - val_loss: 1.0878 - val_acc: 0.5878\n",
            "Epoch 17/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.9948 - acc: 0.6265 - val_loss: 1.0960 - val_acc: 0.5772\n",
            "Epoch 18/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.9765 - acc: 0.6302 - val_loss: 1.0995 - val_acc: 0.5847\n",
            "Epoch 19/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.9511 - acc: 0.6417 - val_loss: 1.1039 - val_acc: 0.5861\n",
            "Epoch 20/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.9399 - acc: 0.6455 - val_loss: 1.0920 - val_acc: 0.5931\n",
            "Epoch 21/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.9116 - acc: 0.6548 - val_loss: 1.1383 - val_acc: 0.5861\n",
            "Epoch 22/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.8831 - acc: 0.6644 - val_loss: 1.1560 - val_acc: 0.5956\n",
            "Epoch 23/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.8691 - acc: 0.6672 - val_loss: 1.1124 - val_acc: 0.5945\n",
            "Epoch 24/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.8473 - acc: 0.6727 - val_loss: 1.1419 - val_acc: 0.5967\n",
            "Epoch 25/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.8294 - acc: 0.6815 - val_loss: 1.1482 - val_acc: 0.5920\n",
            "Epoch 26/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.8088 - acc: 0.6892 - val_loss: 1.1598 - val_acc: 0.6028\n",
            "Epoch 27/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.7923 - acc: 0.6936 - val_loss: 1.1659 - val_acc: 0.6031\n",
            "Epoch 28/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.7647 - acc: 0.7052 - val_loss: 1.2029 - val_acc: 0.5959\n",
            "Epoch 29/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.7478 - acc: 0.7150 - val_loss: 1.2451 - val_acc: 0.5998\n",
            "Epoch 30/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.7328 - acc: 0.7157 - val_loss: 1.1780 - val_acc: 0.6090\n",
            "Epoch 31/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.7106 - acc: 0.7274 - val_loss: 1.2566 - val_acc: 0.6026\n",
            "Epoch 32/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.7035 - acc: 0.7266 - val_loss: 1.2636 - val_acc: 0.6109\n",
            "Epoch 33/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.6864 - acc: 0.7350 - val_loss: 1.3215 - val_acc: 0.6062\n",
            "Epoch 34/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.6733 - acc: 0.7391 - val_loss: 1.2761 - val_acc: 0.6126\n",
            "Epoch 35/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.6475 - acc: 0.7481 - val_loss: 1.3263 - val_acc: 0.6106\n",
            "Epoch 36/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.6383 - acc: 0.7499 - val_loss: 1.3454 - val_acc: 0.6081\n",
            "Epoch 37/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.6180 - acc: 0.7573 - val_loss: 1.3718 - val_acc: 0.6067\n",
            "Epoch 38/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.6132 - acc: 0.7549 - val_loss: 1.3828 - val_acc: 0.6056\n",
            "Epoch 39/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.5953 - acc: 0.7639 - val_loss: 1.4439 - val_acc: 0.6148\n",
            "Epoch 40/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5921 - acc: 0.7633 - val_loss: 1.4093 - val_acc: 0.6040\n",
            "Epoch 41/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.5641 - acc: 0.7761 - val_loss: 1.4269 - val_acc: 0.6129\n",
            "Epoch 42/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.5553 - acc: 0.7798 - val_loss: 1.4583 - val_acc: 0.6095\n",
            "Epoch 43/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5401 - acc: 0.7857 - val_loss: 1.5229 - val_acc: 0.6101\n",
            "Epoch 44/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5372 - acc: 0.7884 - val_loss: 1.4965 - val_acc: 0.6185\n",
            "Epoch 45/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5295 - acc: 0.7871 - val_loss: 1.5362 - val_acc: 0.6182\n",
            "Epoch 46/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5167 - acc: 0.7940 - val_loss: 1.5956 - val_acc: 0.6173\n",
            "Epoch 47/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.5115 - acc: 0.7943 - val_loss: 1.4826 - val_acc: 0.6132\n",
            "Epoch 48/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.5002 - acc: 0.7979 - val_loss: 1.5870 - val_acc: 0.6123\n",
            "Epoch 49/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.4869 - acc: 0.8013 - val_loss: 1.6086 - val_acc: 0.6134\n",
            "Epoch 50/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.4794 - acc: 0.8063 - val_loss: 1.6853 - val_acc: 0.6070\n",
            "Epoch 51/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.4771 - acc: 0.8061 - val_loss: 1.6947 - val_acc: 0.6081\n",
            "Epoch 52/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.4699 - acc: 0.8083 - val_loss: 1.7210 - val_acc: 0.6154\n",
            "Epoch 53/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.4664 - acc: 0.8101 - val_loss: 1.6832 - val_acc: 0.6093\n",
            "Epoch 54/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.4394 - acc: 0.8200 - val_loss: 1.7986 - val_acc: 0.6090\n",
            "Epoch 55/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.4486 - acc: 0.8194 - val_loss: 1.7794 - val_acc: 0.6126\n",
            "Epoch 56/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.4386 - acc: 0.8223 - val_loss: 1.7937 - val_acc: 0.6157\n",
            "Epoch 57/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.4268 - acc: 0.8254 - val_loss: 1.8468 - val_acc: 0.6140\n",
            "Epoch 58/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.4235 - acc: 0.8244 - val_loss: 1.9330 - val_acc: 0.6079\n",
            "Epoch 59/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.4112 - acc: 0.8308 - val_loss: 1.8868 - val_acc: 0.6095\n",
            "Epoch 60/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.4078 - acc: 0.8327 - val_loss: 1.9213 - val_acc: 0.6073\n",
            "Epoch 61/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.4090 - acc: 0.8303 - val_loss: 1.9495 - val_acc: 0.6201\n",
            "Epoch 62/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.4093 - acc: 0.8301 - val_loss: 2.0417 - val_acc: 0.6073\n",
            "Epoch 63/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3971 - acc: 0.8364 - val_loss: 1.9324 - val_acc: 0.6031\n",
            "Epoch 64/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3938 - acc: 0.8345 - val_loss: 1.9816 - val_acc: 0.6109\n",
            "Epoch 65/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.3898 - acc: 0.8381 - val_loss: 2.1266 - val_acc: 0.6059\n",
            "Epoch 66/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3886 - acc: 0.8384 - val_loss: 2.0167 - val_acc: 0.6185\n",
            "Epoch 67/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3796 - acc: 0.8432 - val_loss: 2.0310 - val_acc: 0.6129\n",
            "Epoch 68/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.3771 - acc: 0.8438 - val_loss: 2.2201 - val_acc: 0.6056\n",
            "Epoch 69/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.3656 - acc: 0.8485 - val_loss: 2.1352 - val_acc: 0.6151\n",
            "Epoch 70/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3608 - acc: 0.8529 - val_loss: 2.1153 - val_acc: 0.6157\n",
            "Epoch 71/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3640 - acc: 0.8492 - val_loss: 2.1332 - val_acc: 0.6073\n",
            "Epoch 72/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3536 - acc: 0.8539 - val_loss: 2.1102 - val_acc: 0.6023\n",
            "Epoch 73/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.3564 - acc: 0.8537 - val_loss: 2.1217 - val_acc: 0.6106\n",
            "Epoch 74/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.3527 - acc: 0.8552 - val_loss: 2.1972 - val_acc: 0.6095\n",
            "Epoch 75/200\n",
            "32298/32298 [==============================] - 10s 300us/step - loss: 0.3449 - acc: 0.8549 - val_loss: 2.2118 - val_acc: 0.6087\n",
            "Epoch 76/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.3441 - acc: 0.8550 - val_loss: 2.2394 - val_acc: 0.6145\n",
            "Epoch 77/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.3443 - acc: 0.8572 - val_loss: 2.2065 - val_acc: 0.6185\n",
            "Epoch 78/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3337 - acc: 0.8587 - val_loss: 2.3135 - val_acc: 0.6162\n",
            "Epoch 79/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.3295 - acc: 0.8645 - val_loss: 2.3022 - val_acc: 0.6148\n",
            "Epoch 80/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3288 - acc: 0.8634 - val_loss: 2.3075 - val_acc: 0.6165\n",
            "Epoch 81/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.3452 - acc: 0.8568 - val_loss: 2.3123 - val_acc: 0.6221\n",
            "Epoch 82/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.3263 - acc: 0.8635 - val_loss: 2.3297 - val_acc: 0.6143\n",
            "Epoch 83/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.3278 - acc: 0.8630 - val_loss: 2.3689 - val_acc: 0.6182\n",
            "Epoch 84/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.3151 - acc: 0.8683 - val_loss: 2.3035 - val_acc: 0.6140\n",
            "Epoch 85/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3122 - acc: 0.8685 - val_loss: 2.4745 - val_acc: 0.6154\n",
            "Epoch 86/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3127 - acc: 0.8697 - val_loss: 2.5027 - val_acc: 0.6098\n",
            "Epoch 87/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3064 - acc: 0.8711 - val_loss: 2.4715 - val_acc: 0.6120\n",
            "Epoch 88/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3166 - acc: 0.8680 - val_loss: 2.4146 - val_acc: 0.6129\n",
            "Epoch 89/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3025 - acc: 0.8736 - val_loss: 2.5042 - val_acc: 0.6129\n",
            "Epoch 90/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3013 - acc: 0.8752 - val_loss: 2.5184 - val_acc: 0.6120\n",
            "Epoch 91/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2948 - acc: 0.8782 - val_loss: 2.5430 - val_acc: 0.6143\n",
            "Epoch 92/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3005 - acc: 0.8742 - val_loss: 2.5025 - val_acc: 0.6134\n",
            "Epoch 93/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2966 - acc: 0.8771 - val_loss: 2.5909 - val_acc: 0.6095\n",
            "Epoch 94/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2897 - acc: 0.8808 - val_loss: 2.5440 - val_acc: 0.6034\n",
            "Epoch 95/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.3027 - acc: 0.8726 - val_loss: 2.6198 - val_acc: 0.6026\n",
            "Epoch 96/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2874 - acc: 0.8777 - val_loss: 2.6480 - val_acc: 0.6140\n",
            "Epoch 97/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2919 - acc: 0.8793 - val_loss: 2.5697 - val_acc: 0.6051\n",
            "Epoch 98/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2885 - acc: 0.8797 - val_loss: 2.5937 - val_acc: 0.6037\n",
            "Epoch 99/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2833 - acc: 0.8826 - val_loss: 2.6706 - val_acc: 0.6079\n",
            "Epoch 100/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2807 - acc: 0.8839 - val_loss: 2.5984 - val_acc: 0.6065\n",
            "Epoch 101/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2838 - acc: 0.8810 - val_loss: 2.6762 - val_acc: 0.6101\n",
            "Epoch 102/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2767 - acc: 0.8852 - val_loss: 2.6386 - val_acc: 0.6118\n",
            "Epoch 103/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2800 - acc: 0.8819 - val_loss: 2.6325 - val_acc: 0.6115\n",
            "Epoch 104/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2783 - acc: 0.8845 - val_loss: 2.6494 - val_acc: 0.6067\n",
            "Epoch 105/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2752 - acc: 0.8856 - val_loss: 2.6882 - val_acc: 0.6079\n",
            "Epoch 106/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2711 - acc: 0.8853 - val_loss: 2.7588 - val_acc: 0.6098\n",
            "Epoch 107/200\n",
            "32298/32298 [==============================] - 10s 299us/step - loss: 0.2592 - acc: 0.8934 - val_loss: 2.7880 - val_acc: 0.5998\n",
            "Epoch 108/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2664 - acc: 0.8899 - val_loss: 2.7995 - val_acc: 0.6093\n",
            "Epoch 109/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2582 - acc: 0.8929 - val_loss: 2.7997 - val_acc: 0.6126\n",
            "Epoch 110/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2627 - acc: 0.8901 - val_loss: 2.8622 - val_acc: 0.6026\n",
            "Epoch 111/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2680 - acc: 0.8889 - val_loss: 2.7102 - val_acc: 0.6076\n",
            "Epoch 112/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2662 - acc: 0.8905 - val_loss: 2.9156 - val_acc: 0.6073\n",
            "Epoch 113/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2504 - acc: 0.8963 - val_loss: 2.8705 - val_acc: 0.6123\n",
            "Epoch 114/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2604 - acc: 0.8932 - val_loss: 2.8599 - val_acc: 0.6056\n",
            "Epoch 115/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2529 - acc: 0.8936 - val_loss: 2.9337 - val_acc: 0.6079\n",
            "Epoch 116/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2573 - acc: 0.8940 - val_loss: 2.7864 - val_acc: 0.6118\n",
            "Epoch 117/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2590 - acc: 0.8926 - val_loss: 2.8972 - val_acc: 0.6143\n",
            "Epoch 118/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2488 - acc: 0.8961 - val_loss: 2.8601 - val_acc: 0.6120\n",
            "Epoch 119/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2556 - acc: 0.8936 - val_loss: 2.9439 - val_acc: 0.6054\n",
            "Epoch 120/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2472 - acc: 0.8981 - val_loss: 2.8749 - val_acc: 0.6101\n",
            "Epoch 121/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2438 - acc: 0.8991 - val_loss: 2.8648 - val_acc: 0.6026\n",
            "Epoch 122/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2441 - acc: 0.8980 - val_loss: 2.8704 - val_acc: 0.6112\n",
            "Epoch 123/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2384 - acc: 0.9005 - val_loss: 2.9226 - val_acc: 0.6098\n",
            "Epoch 124/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2388 - acc: 0.9002 - val_loss: 2.8749 - val_acc: 0.5987\n",
            "Epoch 125/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2387 - acc: 0.9012 - val_loss: 2.9093 - val_acc: 0.6048\n",
            "Epoch 126/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2374 - acc: 0.9025 - val_loss: 2.8573 - val_acc: 0.6065\n",
            "Epoch 127/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2353 - acc: 0.9029 - val_loss: 2.9605 - val_acc: 0.6081\n",
            "Epoch 128/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2365 - acc: 0.9034 - val_loss: 2.9065 - val_acc: 0.6012\n",
            "Epoch 129/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2377 - acc: 0.9004 - val_loss: 2.9239 - val_acc: 0.6134\n",
            "Epoch 130/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2318 - acc: 0.9050 - val_loss: 2.9886 - val_acc: 0.6079\n",
            "Epoch 131/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2400 - acc: 0.9018 - val_loss: 3.0418 - val_acc: 0.6176\n",
            "Epoch 132/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2322 - acc: 0.9046 - val_loss: 3.0168 - val_acc: 0.6054\n",
            "Epoch 133/200\n",
            "32298/32298 [==============================] - 10s 297us/step - loss: 0.2277 - acc: 0.9045 - val_loss: 3.0950 - val_acc: 0.6081\n",
            "Epoch 134/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2367 - acc: 0.9027 - val_loss: 2.9944 - val_acc: 0.6056\n",
            "Epoch 135/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2291 - acc: 0.9060 - val_loss: 3.0154 - val_acc: 0.6040\n",
            "Epoch 136/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2317 - acc: 0.9039 - val_loss: 3.0122 - val_acc: 0.6120\n",
            "Epoch 137/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2269 - acc: 0.9058 - val_loss: 3.0834 - val_acc: 0.6112\n",
            "Epoch 138/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2177 - acc: 0.9093 - val_loss: 3.0485 - val_acc: 0.6065\n",
            "Epoch 139/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 0.2250 - acc: 0.9060 - val_loss: 3.2073 - val_acc: 0.6073\n",
            "Epoch 140/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 0.2153 - acc: 0.9113 - val_loss: 3.0027 - val_acc: 0.6045\n",
            "Epoch 141/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2158 - acc: 0.9114 - val_loss: 3.0897 - val_acc: 0.6028\n",
            "Epoch 142/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2217 - acc: 0.9083 - val_loss: 3.2216 - val_acc: 0.6056\n",
            "Epoch 143/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2174 - acc: 0.9112 - val_loss: 3.1230 - val_acc: 0.6143\n",
            "Epoch 144/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.2138 - acc: 0.9119 - val_loss: 3.0914 - val_acc: 0.6042\n",
            "Epoch 145/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.2271 - acc: 0.9082 - val_loss: 3.0692 - val_acc: 0.6098\n",
            "Epoch 146/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2196 - acc: 0.9095 - val_loss: 3.1133 - val_acc: 0.6098\n",
            "Epoch 147/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2064 - acc: 0.9154 - val_loss: 3.2556 - val_acc: 0.6001\n",
            "Epoch 148/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2059 - acc: 0.9141 - val_loss: 3.2032 - val_acc: 0.6023\n",
            "Epoch 149/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2194 - acc: 0.9119 - val_loss: 3.2152 - val_acc: 0.6073\n",
            "Epoch 150/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2095 - acc: 0.9145 - val_loss: 3.1661 - val_acc: 0.6106\n",
            "Epoch 151/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2105 - acc: 0.9121 - val_loss: 3.0894 - val_acc: 0.6001\n",
            "Epoch 152/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1993 - acc: 0.9178 - val_loss: 3.1306 - val_acc: 0.6087\n",
            "Epoch 153/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2059 - acc: 0.9150 - val_loss: 3.3191 - val_acc: 0.6048\n",
            "Epoch 154/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.2135 - acc: 0.9115 - val_loss: 3.1369 - val_acc: 0.6093\n",
            "Epoch 155/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2096 - acc: 0.9131 - val_loss: 3.1358 - val_acc: 0.6143\n",
            "Epoch 156/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2121 - acc: 0.9131 - val_loss: 3.1228 - val_acc: 0.6045\n",
            "Epoch 157/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2108 - acc: 0.9134 - val_loss: 3.2438 - val_acc: 0.6054\n",
            "Epoch 158/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2102 - acc: 0.9134 - val_loss: 3.2129 - val_acc: 0.6120\n",
            "Epoch 159/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.2019 - acc: 0.9163 - val_loss: 3.2436 - val_acc: 0.6095\n",
            "Epoch 160/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2075 - acc: 0.9151 - val_loss: 3.2171 - val_acc: 0.6070\n",
            "Epoch 161/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1954 - acc: 0.9204 - val_loss: 3.2070 - val_acc: 0.6104\n",
            "Epoch 162/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.2009 - acc: 0.9187 - val_loss: 3.2627 - val_acc: 0.6079\n",
            "Epoch 163/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.2006 - acc: 0.9196 - val_loss: 3.2608 - val_acc: 0.6093\n",
            "Epoch 164/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.2036 - acc: 0.9161 - val_loss: 3.2733 - val_acc: 0.6056\n",
            "Epoch 165/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1999 - acc: 0.9191 - val_loss: 3.1544 - val_acc: 0.6126\n",
            "Epoch 166/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1995 - acc: 0.9188 - val_loss: 3.2877 - val_acc: 0.6040\n",
            "Epoch 167/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.2002 - acc: 0.9197 - val_loss: 3.2216 - val_acc: 0.6067\n",
            "Epoch 168/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1913 - acc: 0.9211 - val_loss: 3.2870 - val_acc: 0.6056\n",
            "Epoch 169/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1862 - acc: 0.9261 - val_loss: 3.2875 - val_acc: 0.6081\n",
            "Epoch 170/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.1991 - acc: 0.9190 - val_loss: 3.2871 - val_acc: 0.6054\n",
            "Epoch 171/200\n",
            "32298/32298 [==============================] - 10s 298us/step - loss: 0.1917 - acc: 0.9210 - val_loss: 3.2083 - val_acc: 0.6081\n",
            "Epoch 172/200\n",
            "32298/32298 [==============================] - 10s 296us/step - loss: 0.1895 - acc: 0.9227 - val_loss: 3.2561 - val_acc: 0.6073\n",
            "Epoch 173/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1945 - acc: 0.9203 - val_loss: 3.3573 - val_acc: 0.6020\n",
            "Epoch 174/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.1945 - acc: 0.9206 - val_loss: 3.3475 - val_acc: 0.6042\n",
            "Epoch 175/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1857 - acc: 0.9240 - val_loss: 3.3088 - val_acc: 0.6120\n",
            "Epoch 176/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1898 - acc: 0.9235 - val_loss: 3.3773 - val_acc: 0.6215\n",
            "Epoch 177/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1834 - acc: 0.9252 - val_loss: 3.4434 - val_acc: 0.6093\n",
            "Epoch 178/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1803 - acc: 0.9257 - val_loss: 3.2807 - val_acc: 0.6037\n",
            "Epoch 179/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1821 - acc: 0.9259 - val_loss: 3.4067 - val_acc: 0.6115\n",
            "Epoch 180/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1806 - acc: 0.9260 - val_loss: 3.3451 - val_acc: 0.6101\n",
            "Epoch 181/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.1848 - acc: 0.9251 - val_loss: 3.3317 - val_acc: 0.6187\n",
            "Epoch 182/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1890 - acc: 0.9233 - val_loss: 3.4060 - val_acc: 0.6062\n",
            "Epoch 183/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1808 - acc: 0.9250 - val_loss: 3.4075 - val_acc: 0.6140\n",
            "Epoch 184/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1800 - acc: 0.9274 - val_loss: 3.4107 - val_acc: 0.6090\n",
            "Epoch 185/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1859 - acc: 0.9223 - val_loss: 3.2571 - val_acc: 0.6054\n",
            "Epoch 186/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1781 - acc: 0.9270 - val_loss: 3.2249 - val_acc: 0.6062\n",
            "Epoch 187/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1783 - acc: 0.9280 - val_loss: 3.3214 - val_acc: 0.6123\n",
            "Epoch 188/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1723 - acc: 0.9284 - val_loss: 3.4025 - val_acc: 0.6126\n",
            "Epoch 189/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1893 - acc: 0.9240 - val_loss: 3.3267 - val_acc: 0.6059\n",
            "Epoch 190/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1912 - acc: 0.9227 - val_loss: 3.4880 - val_acc: 0.6026\n",
            "Epoch 191/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1769 - acc: 0.9266 - val_loss: 3.4157 - val_acc: 0.6095\n",
            "Epoch 192/200\n",
            "32298/32298 [==============================] - 9s 294us/step - loss: 0.1730 - acc: 0.9299 - val_loss: 3.3730 - val_acc: 0.6031\n",
            "Epoch 193/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1789 - acc: 0.9275 - val_loss: 3.2665 - val_acc: 0.6106\n",
            "Epoch 194/200\n",
            "32298/32298 [==============================] - 10s 295us/step - loss: 0.1734 - acc: 0.9287 - val_loss: 3.4176 - val_acc: 0.6084\n",
            "Epoch 195/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1877 - acc: 0.9239 - val_loss: 3.4200 - val_acc: 0.6054\n",
            "Epoch 196/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.1780 - acc: 0.9284 - val_loss: 3.3490 - val_acc: 0.6051\n",
            "Epoch 197/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1718 - acc: 0.9306 - val_loss: 3.4381 - val_acc: 0.6042\n",
            "Epoch 198/200\n",
            "32298/32298 [==============================] - 10s 294us/step - loss: 0.1675 - acc: 0.9326 - val_loss: 3.4310 - val_acc: 0.6065\n",
            "Epoch 199/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.1708 - acc: 0.9316 - val_loss: 3.4918 - val_acc: 0.6079\n",
            "Epoch 200/200\n",
            "32298/32298 [==============================] - 9s 293us/step - loss: 0.1649 - acc: 0.9315 - val_loss: 3.4780 - val_acc: 0.6118\n",
            "Test loss: 3.4780489210301018\n",
            "Test accuracy: 0.6117614269788183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtCYmCRLj9fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "ab1fdaa6-2e8a-4f4d-9ecd-3fc541833ad4"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 13179504866963555174, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 9197910189893618559\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 10350872910569264613\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11326753997\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 14506816406449921320\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw1GzNEzFSfu",
        "colab_type": "code",
        "outputId": "bdc582d1-bcb6-4484-8d76-d52927ee9dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRbRY2yiNBGy",
        "colab_type": "text"
      },
      "source": [
        "## Fer2013首次测试\n",
        "\n",
        "日期：2019.8.21\n",
        "\n",
        "----\n",
        "\n",
        "### Python 3 Google Compute Engine 后端(GPU)\n",
        "\n",
        "内存：12.72G\n",
        "\n",
        "每个epoch时间9-10s\n",
        "\n",
        "运行完大约需要34分钟\n",
        "\n",
        "实际运行完成时间1928.73s(32.13min)\n",
        "\n",
        "---\n",
        "\n",
        "### YCKJ2033 \n",
        "CPU: Intel(R) Core(TM) i5-8500 CPU.00GHz 3.00 GHz\n",
        "\n",
        "GPU：无\n",
        "\n",
        "内存：16G\n",
        "\n",
        "每个epoch时间188-192s\n",
        "\n",
        "运行完大约需要10.4小时"
      ]
    }
  ]
}